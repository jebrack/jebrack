---
title: "2025-10-07"
format:
  html: default
  pdf: default
params:
  course: "mc501"
  word_min: 450
  word_max: 500
 
  p3: 'Think about a time you were measured or evaluated—maybe on a test, a performance review, or even a personality quiz. Did the measure feel reliable (consistent)? Did it feel valid (accurate)? Explain your experience and how it relates to the difference between reliability and validity. Why is it essential for a measure to be both? Which one seems more complicated to achieve, and why?'
---

## Choose **one** prompt to answer


> **Prompt C:** `r params$p3`

---

## Response

<!-- RESPONSE-START -->
One of the most memorable times I was evaluated was during high school when I took the SAT. As someone who has grown up with ADHD, I found the experience particularly challenging and frustrating. The SAT is designed to measure academic readiness for college, but for me, it didn’t feel like an accurate reflection of my abilities—especially in the English section. My math score was average, which aligned with my classroom performance, but my English score was significantly below average. This result surprised me because I had always done reasonably well in English classes and felt confident in my reading and writing skills.
After receiving my initial scores, I enrolled in a prep course that focused on test-taking strategies and familiarized me with the types of questions on the SAT. With practice and guidance, I was able to dramatically improve my English score on the retake. This experience highlighted a key issue with the validity of the SAT for someone like me. Validity refers to how well a test measures what it claims to measure. In my case, the SAT didn’t accurately measure my knowledge or potential in English—it measured how well I could perform under timed, high-pressure conditions that are particularly difficult for individuals with ADHD. The improvement in my score after taking a prep course suggests that the test was more about mastering its format than demonstrating true academic ability.
On the other hand, I do believe the SAT is reliable, meaning it produces consistent results under similar conditions. The structure, timing, and scoring are standardized, so if someone takes the test multiple times without any changes in preparation, their scores are likely to be similar. Reliability is important because it ensures fairness in how people are evaluated. However, a test can be reliable without being valid. That’s what I experienced—the SAT consistently measured my ability to take that specific kind of test, but not necessarily my actual knowledge or potential.
This distinction between reliability and validity is crucial in any kind of evaluation. A measure needs to be both consistent and accurate to be truly useful. If a test is reliable but not valid, it may consistently give misleading results. If it’s valid but not reliable, the results may vary too much to be trusted. In educational and psychological assessments, both qualities are essential to ensure fair and meaningful evaluations.
Between the two, I think validity is more difficult to achieve. Reliability can often be ensured through standardization and repetition, but validity requires a deeper understanding of what is being measured and how different factors—like learning differences, cultural background, or test anxiety—can affect outcomes. Designing a test that accurately reflects diverse abilities and experiences is a complex challenge, especially when trying to apply it broadly across populations.

<!-- RESPONSE-END -->

---

## Word Count & Range Check

```{r}
#| echo: false
#| message: false
#| warning: false
get_response_text <- function() {
  f <- knitr::current_input()
  if (is.null(f) || !file.exists(f)) return("")
  x <- readLines(f, warn = FALSE)
  # Find the lines that EXACTLY match the start/end markers
  s <- grep("^<!-- RESPONSE-START -->$", x)
  e <- grep("^<!-- RESPONSE-END -->$", x)
  if (length(s) != 1 || length(e) != 1 || e <= s) return("")
  paste(x[(s + 1L):(e - 1L)], collapse = "\n")
}
count_words <- function(txt) {
  # Remove code blocks and inline code before counting
  txt <- gsub("```[\\s\\S]*?```", " ", txt, perl = TRUE)
  txt <- gsub("`[^`]*`", " ", txt, perl = TRUE)
  # Keep letters, numbers, spaces, hyphens, and apostrophes
  txt <- gsub("[^\\p{L}\\p{N}\\s'-]", " ", txt, perl = TRUE)
  # Split by whitespace and count non-empty words
  words <- unlist(strsplit(txt, "\\s+", perl = TRUE))
  words <- words[nzchar(words)]
  length(words)
}
txt <- get_response_text()
n <- count_words(txt)
minw <- as.integer(params$word_min)
maxw <- as.integer(params$word_max)
in_range <- n >= minw && n <= maxw
cat(sprintf("**Word count:** %d  \n", n))
cat(sprintf("**Required range (%s):** %d–%d words  \n",
            toupper(params$course), minw, maxw))
cat(if (in_range) "**Status:** ✅ In range\n" else "**Status:** ❌ Out of range\n")
```
